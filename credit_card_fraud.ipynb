{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Evaluation\n",
    "\n",
    "Here, a number of models have been trained and how effectively they predict fraud cases using data based on [this Kaggle dataset] (https://www.kaggle.com/dalpozz/creditcardfraud).\n",
    "\n",
    "Each line in `fraud_data.csv` corresponds to a credit card transaction. Features include confidential variables `V1` to` V28` and also` Amount`, which is the value of the transaction.\n",
    "'Amount' => Amount of each transaction\n",
    "\n",
    "The target is stored in the `class` column, where the value 1 corresponds to a fraud instance and 0 corresponds to a non-fraud instance.\n",
    "\n",
    "Target column -> 'class'\n",
    "Fraud => 1\n",
    "No Fraud => 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Import the data from `fraud_data.csv`. What percentage of the observations in the dataset are instances of fraud?\n",
    "\n",
    "*This function should return a float between 0 and 1.* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_one():\n",
    "    \n",
    "    # Your code here\n",
    "    data_frame = pd.read_csv('fraud_data.csv')\n",
    "    X, y = data_frame.drop('Class', axis=1), data_frame.Class;\n",
    "    \n",
    "    result = len(y[y==1]) / (len(y[y==1]) + len(y[y==0]))\n",
    "    \n",
    "    return result\n",
    "answer_one()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use X_train, X_test, y_train, y_test for all of the following questions\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv('fraud_data.csv')\n",
    "\n",
    "X = df.iloc[:,:-1]\n",
    "y = df.iloc[:,-1]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Using `X_train`, `X_test`, `y_train`, and `y_test` (as defined above), train a dummy classifier that classifies everything as the majority class of the training data. What is the accuracy of this classifier? What is the recall?\n",
    "\n",
    "*This function should a return a tuple with two floats, i.e. `(accuracy score, recall score)`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_two():\n",
    "    from sklearn.dummy import DummyClassifier\n",
    "    from sklearn.metrics import recall_score,accuracy_score\n",
    "\n",
    "    \n",
    "    # Creating a dataset with imbalanced binary classes:  \n",
    "    # Negative class (0) is 'not digit 1' - Not Fraud\n",
    "    # Positive class (1) is 'digit 1' - Fraud\n",
    "    y_binary_imbalanced = y.copy()\n",
    "    y_binary_imbalanced[y_binary_imbalanced != 1] = 0\n",
    "\n",
    "    original =  y\n",
    "    imbalanced = y_binary_imbalanced\n",
    "    \n",
    "    #Agora vamos criar uma partição de teste e trainamento neste conjunto de desequilíbrios.\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y_binary_imbalanced, random_state=0)\n",
    "    \n",
    "    # Negative class (0) is most frequent\n",
    "    #A classe negativa (0) é mais frequente\n",
    "    dummy_majority = DummyClassifier(strategy = 'most_frequent').fit(X_train, y_train)\n",
    "    \n",
    "    # Therefore the dummy 'most_frequent' classifier always predicts class 0\n",
    "    # Portanto, o classificador fictício 'most_frequent' sempre prediz a classe 0\n",
    "    y_dummy_predictions = dummy_majority.predict(X_test)\n",
    "    \n",
    "    #calculation of scores\n",
    "    accuracy = (accuracy_score(y_test, y_dummy_predictions))\n",
    "    recall = (recall_score(y_test, y_dummy_predictions))\n",
    "    \n",
    "    \n",
    "    return (accuracy,recall)\n",
    "answer_two()\n",
    "\n",
    "#### Accuracy ####\n",
    "#Accuracy in classification problems is the number of predictions\n",
    "# corrects made by the model in all types of predictions made.\n",
    "# 0.985 is almost perfect, HOWEVER -> Accuracy should NEVER be\n",
    "# used as a measure when classes of variables\n",
    "# destination in the data are majority of a class.\n",
    "\n",
    "####Recall#####\n",
    "# basically, if we want to focus more on\n",
    "# minimizing false negatives, we would like our recall\n",
    "# be as close to 100% as possible\n",
    "# Recall is a measure that indicates what proportion\n",
    "# of patients who actually had cancer was diagnosed by the algorithm as having cancer\n",
    "# 0 so it's really, really bad.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Using X_train, X_test, y_train, y_test (as defined above), train a SVC classifer using the default parameters. What is the accuracy, recall, and precision of this classifier?\n",
    "\n",
    "*This function should a return a tuple with three floats, i.e. `(accuracy score, recall score, precision score)`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_three():\n",
    "    from sklearn.metrics import accuracy_score, recall_score, precision_score\n",
    "    from sklearn.svm import SVC\n",
    "    \n",
    "    #Model SVC with default parameters. SVC without mencions of kernel, the default is rbf\n",
    "    svc_model = SVC().fit(X_train, y_train)\n",
    "    \n",
    "    #Threshold = using 0.5 by default\n",
    "    svm_predicted = svc_model.predict(X_test)\n",
    "\n",
    "    #Scores\n",
    "    accuracy = accuracy_score(y_test, svm_predicted)\n",
    "    recall = recall_score(y_test, svm_predicted)\n",
    "    precision = precision_score(y_test, svm_predicted)\n",
    "                  \n",
    "    \n",
    "    return (accuracy,recall,precision)\n",
    "answer_three()\n",
    "\n",
    "##Precision##\n",
    "#Precisão é ser preciso. Portanto, mesmo que tenhamos conseguido \n",
    "#capturar apenas um caso de câncer e o tenhamos capturado corretamente, somos 100% precisos.\n",
    "#MINIMIZAR FALSOS POSITIVOS: utilize PRECISION O MAIS PROXIMO POSSIVEL DE 100%,\n",
    "\n",
    "##Recall###\n",
    "#FALSOS NEGATIVOS: utilize RECALL O MAIS PROXIMO POSSIVEL DE 100%\n",
    "#Nosso Recall está baixo, quer dizer que existem falsos negativos no modelo\n",
    "\n",
    "##Acuracy###\n",
    "#percentual de acertos em classificação, mas isto não pode ser usado em classes desequilibradas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Using the SVC classifier with parameters `{'C': 1e9, 'gamma': 1e-07}`, what is the confusion matrix when using a threshold of -220 on the decision function. Use X_test and y_test.\n",
    "\n",
    "*This function should return a confusion matrix, a 2x2 numpy array with 4 integers.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5320,   24],\n",
       "       [  14,   66]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def answer_four():\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    from sklearn.svm import SVC\n",
    "    from sklearn.calibration import CalibratedClassifierCV\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    #SVC without mencions of kernel, the default is rbf\n",
    "    svc = SVC(C=1e9, gamma=1e-07).fit(X_train, y_train)\n",
    "    \n",
    "    #decision_function scores: Predict confidence scores for samples\n",
    "    #Quando é fornecido um conjunto de pontos de teste, o método decision_function fornece \n",
    "    #para cada um um valor de pontuação do classificador que indica com que confiança o \n",
    "    #classificador prediz a classe positiva. Portanto, haverá pontuações positivas de grande \n",
    "    #magnitude para esses pontos\n",
    "    #ou ele prevê uma classe negativa; haverá pontuações negativas de grande magnitude para \n",
    "    #pontos negativos.\n",
    "    y_score = svc.decision_function(X_test)\n",
    "    \n",
    "    #Set a threshold -220\n",
    "    y_score = np.where(y_score > -220, 1, 0)\n",
    "    conf_matrix = confusion_matrix(y_test, y_score)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    #### threshold ###\n",
    "    # input threshold in the model after trained this model\n",
    "    # threshold is a threshold of separation of class\n",
    "    # see more in https://towardsdatascience.com/fine-tuning-a-classifier-in-scikit-learn-66e048c21e65\n",
    "    # process predict_proba\n",
    "    # Predicts the odds\n",
    "    # Choose the class most likely\n",
    "    # There is a 0.5 rating limit\n",
    "    # Class 1 is predicted if the probability is greater than 0.5\n",
    "    # Class 0 is predicted if the probability is <0.5\n",
    "    \n",
    "    \n",
    "    return conf_matrix\n",
    "answer_four()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Train a logisitic regression classifier with default parameters using X_train and y_train.\n",
    "\n",
    "For the logisitic regression classifier, create a precision recall curve and a roc curve using y_test and the probability estimates for X_test (probability it is fraud).\n",
    "\n",
    "Looking at the precision recall curve, what is the recall when the precision is `0.75`?\n",
    "\n",
    "Looking at the roc curve, what is the true positive rate when the false positive rate is `0.16`?\n",
    "\n",
    "*This function should return a tuple with two floats, i.e. `(recall, true positive rate)`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.825, 0.9375)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "def answer_five():\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.metrics import roc_curve, auc , roc_auc_score, precision_recall_curve, f1_score\n",
    "    #import matplotlib.pyplot as plt\n",
    "   \n",
    "\n",
    "    lr = LogisticRegression().fit(X_train, y_train)\n",
    "    y_score = lr.decision_function(X_test)    \n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_score)\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_score)\n",
    " \n",
    "    #Looking at the precision recall curve, what is the recall when the precision is 0.75?\n",
    "    df_ACU = pd.DataFrame()\n",
    "    df_ACU['precision'] = precision\n",
    "    df_ACU['recall'] = recall\n",
    "    search_recall = df_ACU.precision[df_ACU.precision == 0.75]\n",
    "    searchedQ1 = df_ACU.iloc[search_recall.index]\n",
    "    searchedQ1 = float(searchedQ1['recall'])\n",
    "    \n",
    "    #Looking at the roc curve, what is the true positive rate when the false positive rate is 0.16?\n",
    "    df_ROC = pd.DataFrame()\n",
    "    df_ROC['true_positive_rate'] = tpr\n",
    "    df_ROC['false_positive_rate'] = fpr\n",
    "    search_tpr = df_ROC.false_positive_rate[df_ROC.false_positive_rate == 0.16]\n",
    "    if len(search_tpr) == 0:\n",
    "        searchedQ2 = tpr[np.argmin(abs(fpr - 0.16))]\n",
    "        \n",
    "        #Does not exist df_ROC.false_positive_rate == 0.16\n",
    "        #To answer the original question, you can either interpolate or choose the closest value.\n",
    "        #abs Esta função obtém o valor absoluto do seu argumento, o módulo.\n",
    "    else:\n",
    "        searchedQ2 = df_ROC.iloc[search_tpr.index]\n",
    "\n",
    "    \n",
    "    return (searchedQ1,searchedQ2)\n",
    "\n",
    "answer_five()\n",
    "\n",
    "\n",
    "\n",
    "###ROC CURVES####\n",
    "\n",
    "\n",
    "### Precision recall curve ####\n",
    "# Accuracy is a ratio between the number of true positives divided by the sum\n",
    "# true positives and false positives. He describes\n",
    "# as a model is good at predicting the positive class.\n",
    "# Accuracy is referred to as the positive predictive value\n",
    "\n",
    "# RECALL is calculated as the ratio of the number of true positives divided\n",
    "# the sum of true positives and false negatives. Remember is the same as sensitivity.\n",
    "\n",
    "\n",
    "# OBBSERVAR PRECISION and RECALL is useful in cases where there is an imbalance in the observations between\n",
    "# two classes. Specifically,\n",
    "# there are many examples of no event (class 0) and just a few examples of an event (class 1).\n",
    "\n",
    "# A recovery precision curve is a graph of precision (y-axis) and recovery (x-axis) for different limits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Perform a grid search over the parameters listed below for a Logisitic Regression classifier, using recall for scoring and the default 3-fold cross validation.\n",
    "\n",
    "`'penalty': ['l1', 'l2']`\n",
    "\n",
    "`'C':[0.01, 0.1, 1, 10, 100]`\n",
    "\n",
    "From `.cv_results_`, create an array of the mean test scores of each parameter combination. i.e.\n",
    "\n",
    "|      \t| `l1` \t| `l2` \t|\n",
    "|:----:\t|----\t|----\t|\n",
    "| **`0.01`** \t|    ?\t|   ? \t|\n",
    "| **`0.1`**  \t|    ?\t|   ? \t|\n",
    "| **`1`**    \t|    ?\t|   ? \t|\n",
    "| **`10`**   \t|    ?\t|   ? \t|\n",
    "| **`100`**   \t|    ?\t|   ? \t|\n",
    "\n",
    "<br>\n",
    "\n",
    "*This function should return a 5 by 2 numpy array with 10 floats.* \n",
    "\n",
    "*Note: do not return a DataFrame, just the values denoted by '?' above in a numpy array. You might need to reshape your raw result to meet the format we are looking for.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.66666667,  0.76086957],\n",
       "       [ 0.80072464,  0.80434783],\n",
       "       [ 0.8115942 ,  0.8115942 ],\n",
       "       [ 0.80797101,  0.8115942 ],\n",
       "       [ 0.80797101,  0.80797101]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def answer_six():    \n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.model_selection import cross_val_score\n",
    " \n",
    "    grid_values = {'penalty': ['l1', 'l2'], 'C': [0.01, 0.1, 1, 10, 100]}\n",
    "   \n",
    "   \n",
    "    #train de model with many parameters for \"C\" and penalty='l1'\n",
    "\n",
    "    lr = LogisticRegression()\n",
    "    # Usamos o GridSearchCV para encontrar o valor da gama que otimiza uma determinada métrica de avaliação\n",
    "    grid_lr_recall = GridSearchCV(lr, param_grid = grid_values, cv=3, scoring = 'recall')\n",
    "    grid_lr_recall.fit(X_train, y_train)\n",
    "    y_decision_fn_scores_recall = grid_lr_recall.decision_function(X_test)\n",
    "    \n",
    "    \n",
    "   \n",
    "        \n",
    "       \n",
    "    ##The precision, recall, and accuracy scores for every combination \n",
    "    #of the parameters in param_grid are stored in cv_results_\n",
    "    CVresults = []\n",
    "    CVresults = pd.DataFrame(grid_lr_recall.cv_results_)\n",
    "    \n",
    "    #test scores and mean of them\n",
    "    split_test_scores = np.vstack((CVresults['split0_test_score'], CVresults['split1_test_score'], CVresults['split2_test_score']))\n",
    "    mean_scores = split_test_scores.mean(axis=0).reshape(5, 2)\n",
    "    \n",
    "    \n",
    "        \n",
    "    # The L1 standard (Mean absolute error - Ridge) which is calculated as the sum of the absolute values ​​of the target and predicted vector.\n",
    "    # L1 is most used in time series\n",
    "    # The L1 standard of a vector can be calculated in NumPy using the norm () function with a parameter to specify the order of the standard,\n",
    "    # in this case 1 l1 = norm (a, 1), where a is the array\n",
    "    \n",
    "    \n",
    "    # The L2 standard (mean square error - Lasso) which is calculated as the square root of the sum of the vector values ​​to the target and predicted square.\n",
    "    # The L2 norm of a vector can be calculated in NumPy using the norm () function\n",
    "    # with standard parameters. l2 = norm (a), where a is the array\n",
    "    \n",
    "    \n",
    "    return mean_scores\n",
    "answer_six()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "python-machine-learning",
   "graded_item_id": "5yX9Z",
   "launcher_item_id": "eqnV3",
   "part_id": "Msnj0"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
